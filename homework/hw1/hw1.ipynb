{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\\begin{center}\n",
    "\\begin{huge}\n",
    "MCIS6273 Data Mining (Prof. Maull) / Fall 2023 / HW1\n",
    "\\end{huge}\n",
    "\\end{center}\n",
    "\n",
    "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n",
    "|:---------------:|:--------:|:---------------:|\n",
    "| 20 | Tuesday, Oct 24 @ Midnight | _up to_ 24 hours |\n",
    "\n",
    "\n",
    "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n",
    "\n",
    "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n",
    "\n",
    "## OBJECTIVES\n",
    "* Learn more about data science tools in the wild for practitioners\n",
    "\n",
    "* Perform data engineering  in Pandas\n",
    "\n",
    "* Perform exploratory data analysis (EDA) in Pandas\n",
    "\n",
    "* Perform pattern mining with the `mlxtend` library\n",
    "\n",
    "## WHAT TO TURN IN\n",
    "You are being encouraged to turn the assignment in using the provided\n",
    "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n",
    "`homework/hw0`.   Put all of your files in that directory.  Then tar that directory,\n",
    "rename it with your name as the first part of the filename (e.g. `maull_hw0_files.zip`), then\n",
    "download it to your local machine, then upload the `.zip` to Blackboard.\n",
    "\n",
    "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n",
    "on the basics of using tar in Linux.\n",
    "\n",
    "If you choose not to use the provided notebook, you will still need to turn in a\n",
    "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n",
    "this homework.\n",
    "\n",
    "\n",
    "## ASSIGNMENT TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (20%) Learn more about data science tools in the wild for practitioners \n",
    "\n",
    "Lot's of interesting things are going on in the data \n",
    "science landscape.  You should make sure that\n",
    "you are aware of innovative changes in the field and \n",
    "how data is being used to move and shake even the\n",
    "most tech-resistant industries.\n",
    "\n",
    "You will listen to the approx. 38 minute podcast [Making Data Simple](https://player.fm/series/making-data-simple) **September 13, 2023**: _Data-Driven Apartment Innovation: A Conversation with Mike Kaeding | Part 1_ featuring an \n",
    "interview with Mike Kaeding, CEO of [Norhart](https://norhart.com),\n",
    "who goes into detail about innovation in the apartment industry.\n",
    "\n",
    "You can listen to / watch the show from one of the links below:\n",
    "\n",
    "* (main page) [Player.fm: Making Data Simple | _Data-Driven Apartment Innovation: A Conversation with Mike Kaeding | Part 1_ ](https://player.fm/series/making-data-simple/data-driven-apartment-innovation-a-conversation-with-mike-kaeding-part-1)\n",
    "* (mp3 direct) [MP3 file direct download](https://audio.buzzsprout.com/wwmajkmvz2edu2ihrsiw7rrgwoul?response-content-disposition=inline&)\n",
    "* [Apple Podcasts](https://podcasts.apple.com/us/podcast/data-driven-apartment-innovation-a-conversation/id605818735?i=1000627721356)\n",
    "* [Spotify](https://open.spotify.com/episode/7q5Q1HvXeSstPq2saKLC7H)\n",
    "\n",
    "**&#167; Task:**  Listen to the podcast / watch the video and write a 3-5 sentence \n",
    "_reaction_ to the podcast.  State in your own words what you learned,\n",
    "what expanding your knowledge of the topic and what you found _interesting_\n",
    "about the information you received.\n",
    "\n",
    "\n",
    "**&#167; Task:**  What did you learn about data, data science or data mining that you found surprising? (Please no more than 3 sentenses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Answer\n",
    "\n",
    "## Listen to the podcast / watch the video and write a 3-5 sentence _reaction_ to the podcast.\n",
    "The video discusses the American housing affordability crisis. I was particularly impressed by the utilization of data-derived values to enhance work quality. The efficiency demonstrated in completing one unit in just 5 hours was astonishing. Furthermore, it was evident that data-driven technologies are playing a transformative role in the construction industry. Overall, the podcast significantly expanded my comprehension of how data science has the potential to disrupt even traditional sectors.\n",
    "\n",
    "## State in your own words what you learned, what expanding your knowledge of the topic and what you found _interesting_ about the information you received.\n",
    "From the podcast, I learned that data science is not limited to traditional tech sectors; it has a profound impact on diverse fields. I found it intriguing how he utilized data to establish a well-defined set of principles, without necessarily anticipating his team's ongoing improvement.\n",
    "\n",
    "## What did you learn about data, data science or data mining that you found surprising?\n",
    "In a surprising revelation, I've learned that data mining offers valuable assistance to various industries in enhancing their operations. Proper utilization of historical data can significantly boost success, potentially leading to 2-10 times greater efficiency, as emphasized by the speaker. Furthermore, data mining not only imparts knowledge on \"how\" to perform tasks but also guides \"what\" and \"when\" to take specific actions, providing a comprehensive understanding of data's potential impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (25%) Perform data engineering  in Pandas \n",
    "\n",
    "As data scientists, especially in a small organization, you will\n",
    "be tasked with doing exploratory analysis of data.  Sometimes\n",
    "you will not know much about the data and will need to \n",
    "warm up to it so that you can then choose appropriate \n",
    "secondary methods of analysis.  I often call this \n",
    "\"taking the data out for coffee\" to get to know it a little\n",
    "better.\n",
    "\n",
    "Often times, basic statistical tools go a long way to quickly\n",
    "size up your data and understand which tools make the most\n",
    "sense moving forward.\n",
    "\n",
    "In this part of the assignment we will be working with a very interesting\n",
    "dataset from\n",
    "the University of Arkansas, Fayetteville published just 6 weeks ago on\n",
    "August 13, 2023:\n",
    "\n",
    "> Johansson, Emily (2023). Mammalian Camera Trap Data; Northwest Arkansas [Dataset]. \n",
    "Dryad. [https://doi.org/10.5061/dryad.kd51c5bb6](https://doi.org/10.5061/dryad.kd51c5bb6)\n",
    "\n",
    "This dataset includes the camera trap data from several months of data \n",
    "over two years, mostly during the late spring and summer months.  A\n",
    "_camera trap_ is \n",
    "\n",
    "> \"a device usually comprised of a DSLR camera, \n",
    "equipped with either infrared or motion sensors, and\n",
    "possibly an external flash, set up on a tripod or\n",
    "secured to a tree; When an animal, unaware of the setup,\n",
    "crosses the sensor's path, the camera captures\n",
    "a natural moment of the creature in its routine, \n",
    "without disrupting its life.\"\n",
    "\n",
    "**Please read short the abstract and description of this dataset so \n",
    "you can learn more about it.**  We will be uncovering some\n",
    "basic relationships in the data, which will later help us understand\n",
    "which algorithms we might find interesting. \n",
    "\n",
    "**&#167; Task:**  (_Data Extraction, Selection and Transformation_)\n",
    "\n",
    "The data is mostly in the form we will be working \n",
    "with it, but you will need to do some transformation\n",
    "so that we can work with it in the next part a little\n",
    "more easily.\n",
    "\n",
    "Your Python program / notebook must do the following:\n",
    "\n",
    "(1) fetch to your local file system on Jupyter Hub \n",
    "the main remote data file (there is only one given above)\n",
    "(2)  once you have fetched the file, make 6 new CSV files from it which \n",
    "will act in a similar way as binary databases (similar to those you have read about\n",
    "in Zaki, Ch. 8 and in lecture).  We are going to use these in the next \n",
    "part:\n",
    "\n",
    "* **file 1 (`dataset_2000_2359.csv`)**:\n",
    "\n",
    "> group all rows by date, filter on only the times of day\n",
    " between 2000 (8p) and 2359 (1159p), the columns should be the \n",
    " species and the data instance should be `True` if there are\n",
    " 1 or more of the species, otherwise `False`\n",
    "\n",
    "* **file 2 (`dataset_0000_0459.csv`)**: \n",
    "\n",
    "> similar to file 1 group by date, but the\n",
    "times of day will be 0000 (midnight) and 0459 (459a)\n",
    "\n",
    "* **file  (`dataset_2000_2359_urban.csv`)**: \n",
    "\n",
    "> use file 1 and filter on the columns which have \n",
    "`Forest Cover within 1.5km of camera (km^2)` value less\n",
    "than ($<$) 0.30.  \n",
    "\n",
    "* **file 4 (`dataset_2000_2359_rural.csv`)**: \n",
    "\n",
    "> use file 1 and filter on the columns which have \n",
    "`Forest Cover within 1.5km of camera (km^2)` value greater\n",
    "than or equal to ($\\ge$) 0.30. \n",
    "\n",
    "* **file 5 (`dataset_0000_0459_urban.csv`)**: \n",
    "\n",
    "> similar to file 3, except use file 2 instead of file 1 \n",
    "\n",
    "* **file 6 (`dataset_0000_0459_rural.csv`)**: \n",
    "\n",
    "> similar to file 4, except use file 2 instead of file 1\n",
    "\n",
    "You will (minimally) need to study the following Pandas functions to complete this task:\n",
    "\n",
    "* [`to_csv()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html)\n",
    "* [`groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)\n",
    "* [`unstack()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html)\n",
    "* [`fillna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)\n",
    "* [`map()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html)\n",
    "\n",
    "You do not want to overthink this, but you  **must use the filenames provided**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_138/2525546392.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Time of Detection'] = pd.to_datetime(df['Time of Detection']).dt.time\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the main remote data file into a DataFrame\n",
    "df = pd.read_csv('source.csv')\n",
    "\n",
    "df['Time of Detection'] = pd.to_datetime(df['Time of Detection']).dt.time\n",
    "df['Hour of Detection'] = df['Time of Detection'].apply(lambda t: t.hour)\n",
    "\n",
    "# 1st CSV\n",
    "# Filter rows where the time is between 20:00 (8:00 PM) and 23:59 (11:59 PM)\n",
    "filtered_data1 = df[(df['Time of Detection'] >= pd.to_datetime('20:00:00').time()) & (df['Time of Detection'] <= pd.to_datetime('23:59:59').time())]\n",
    "# Group by species and date, count, unstack and Fill binary\n",
    "filtered_data1 = filtered_data1.groupby(['Date of Detection', 'Species Detected', 'Hour of Detection']).count().loc[:, 'Yard'].unstack().fillna(False).map(lambda x: not x).map(lambda x: not x)\n",
    "\n",
    "# Save the result to dataset_2000_2359.csv fileS\n",
    "filtered_data1.to_csv('dataset_2000_2359.csv', index=True)\n",
    "\n",
    "# 2nd CSV\n",
    "# Filter rows where the time is between 00:00 (00:00 AM) and 04:59 (04:59 AM)\n",
    "filtered_data2 = df[(df['Time of Detection'] >= pd.to_datetime('00:00:00').time()) & (df['Time of Detection'] <= pd.to_datetime('04:59:59').time())]\n",
    "# Group by species and date, count, unstack and Fill binary\n",
    "filtered_data2 = filtered_data2.groupby(['Date of Detection', 'Species Detected', 'Hour of Detection']).count().loc[:, 'Yard'].unstack().fillna(False).map(lambda x: not x).map(lambda x: not x)\n",
    "\n",
    "# Save the result to dataset_0000_0459.csv fileS\n",
    "filtered_data2.to_csv('dataset_0000_0459.csv', index=True)\n",
    "\n",
    "# 3rd CSV\n",
    "# Filter rows where the time is between 20:00 (8:00 PM) and 23:59 (11:59 PM) and Forest Cover within 1.5km of camera (km^2) less than 0.3\n",
    "filtered_data3 = df[(df['Time of Detection'] >= pd.to_datetime('20:00:00').time()) & (df['Time of Detection'] <= pd.to_datetime('23:59:59').time()) & (df['Forest Cover within 1.5km of camera (km^2)'] < 0.3)]\n",
    "# Group by species and date, count, unstack and Fill binary\n",
    "filtered_data3 = filtered_data3.groupby(['Date of Detection', 'Species Detected', 'Hour of Detection']).count().loc[:, 'Yard'].unstack().fillna(False).map(lambda x: not x).map(lambda x: not x)\n",
    "\n",
    "# Save the result to dataset_2000_2359_urban.csv fileS\n",
    "filtered_data3.to_csv('dataset_2000_2359_urban.csv', index=True)\n",
    "\n",
    "# 4th CSV\n",
    "# Filter rows where the time is between 20:00 (8:00 PM) and 23:59 (11:59 PM) and Forest Cover within 1.5km of camera (km^2) greater than or equal to 0.3\n",
    "filtered_data4 = df[(df['Time of Detection'] >= pd.to_datetime('20:00:00').time()) & (df['Time of Detection'] <= pd.to_datetime('23:59:59').time()) & (df['Forest Cover within 1.5km of camera (km^2)'] >= 0.3)]\n",
    "# Group by species and date, count, unstack and Fill binary\n",
    "filtered_data4 = filtered_data4.groupby(['Date of Detection', 'Species Detected', 'Hour of Detection']).count().loc[:, 'Yard'].unstack().fillna(False).map(lambda x: not x).map(lambda x: not x)\n",
    "\n",
    "# Save the result to dataset_2000_2359_rural.csv fileS\n",
    "filtered_data4.to_csv('dataset_2000_2359_rural.csv', index=True)\n",
    "\n",
    "\n",
    "# 5th CSV\n",
    "# Filter rows where the time is between 00:00 (00:00 AM) and 04:59 (04:59 AM) and and Forest Cover within 1.5km of camera (km^2) less than 0.3\n",
    "filtered_data5 = df[(df['Time of Detection'] >= pd.to_datetime('00:00:00').time()) & (df['Time of Detection'] <= pd.to_datetime('04:59:59').time()) & (df['Forest Cover within 1.5km of camera (km^2)'] < 0.3)]\n",
    "# Group by species and date, count, unstack and Fill binary\n",
    "filtered_data5 = filtered_data5.groupby(['Date of Detection', 'Species Detected', 'Hour of Detection']).count().loc[:, 'Yard'].unstack().fillna(False).map(lambda x: not x).map(lambda x: not x)\n",
    "\n",
    "# Save the result to dataset_0000_0459_urban.csv fileS\n",
    "filtered_data5.to_csv('dataset_0000_0459_urban.csv', index=True)\n",
    "\n",
    "\n",
    "# 6th CSV\n",
    "# Filter rows where the time is between 00:00 (00:00 AM) and 04:59 (04:59 AM) and and Forest Cover within 1.5km of camera (km^2) greater than or equal to 0.3\n",
    "filtered_data6 = df[(df['Time of Detection'] >= pd.to_datetime('00:00:00').time()) & (df['Time of Detection'] <= pd.to_datetime('04:59:59').time()) & (df['Forest Cover within 1.5km of camera (km^2)'] >= 0.3)]\n",
    "# Group by species and date, count, unstack and Fill binary\n",
    "filtered_data6 = filtered_data6.groupby(['Date of Detection', 'Species Detected', 'Hour of Detection']).count().loc[:, 'Yard'].unstack().fillna(False).map(lambda x: not x).map(lambda x: not x)\n",
    "\n",
    "# Save the result to dataset_0000_0459_rural.csv fileS\n",
    "filtered_data6.to_csv('dataset_0000_0459_rural.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (25%) Perform exploratory data analysis (EDA) in Pandas \n",
    "\n",
    "Now that we have data, let's perform additional analysis on it.\n",
    "\n",
    "We want to get some ideas about what's in the data, \n",
    "and for now, we will not come back to the files we\n",
    "generated in the prior part until the next\n",
    "part of this homework.\n",
    "\n",
    "Let's find out about our data to learn a few \n",
    "things about what is there.\n",
    "\n",
    "You will need to use the original dataset (not\n",
    "the one's you just created) to answer the following\n",
    "questions (each is work 2 points).\n",
    "\n",
    "You will (minimally) need to study the following Pandas functions to complete this task:\n",
    "* [`groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)\n",
    "* [`unstack()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html)\n",
    "* [`mode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mode.html)\n",
    "\n",
    "You will also need to use the Seaborn (SNS) heatmap function here:\n",
    "\n",
    "* [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n",
    "\n",
    "In your notebook, you will also need to execute to install seaborn:\n",
    "\n",
    "```bash\n",
    "  !conda install -y seaborn -c conda-forge # you must use -y \n",
    "```\n",
    "\n",
    "Each question **must be accompanied by the corresponding\n",
    "Pandas code to earn full credit**:\n",
    "\n",
    "**&#167; Task:**  What are the top 5 species detected over the entire dataset?\n",
    "\n",
    "\n",
    "**&#167; Task:**  Which 5 species over all data are most frequent at midnight (12am)?\n",
    "\n",
    "\n",
    "**&#167; Task:**  Use the library `sns.heatmap()` to generate a heatmap with \n",
    "`Species Detected` on the $y$-axis and `Hour of Detection`\n",
    "on the $x$-axis. \n",
    "\n",
    "\n",
    "**&#167; Task:**  Compare the result of your prior answer with running\n",
    "`heatmap()` with the `robust=True` parameter.  Which \n",
    "of the two output graphs looks more interpretable. Why?\n",
    "\n",
    "\n",
    "**&#167; Task:**  Compute the _mode_ of all species in the dataset. \n",
    "\n",
    "\n",
    "**&#167; Task:**  Compute the probabilities of all species, but make two\n",
    "columns: (1) with those in the group with forest\n",
    "density $<$ 0.3 and (2) those with forest density $\\ge$ 0.3.\n",
    "Label them \"urban\" and \"rural\".\n",
    "\n",
    "Your table will look something like this:\n",
    "\n",
    "| Species Detected      |      rural |      urban |\n",
    "|:----------------------|-----------:|-----------:|\n",
    "| Virginia Opossum      | 0.034703   | 0.0295551  |\n",
    "| Groundhog             | 0.00567261 | 0.0122004  |\n",
    "| Fox Squirrel          | 0.00155719 | 0.00117891 |\n",
    "| Nine-banded Armadillo | 0.0635109  | 0.0263474  |\n",
    "| Person                | 0.0537229  | 0.10199    |\n",
    "| ...     | ...   | ...   |\n",
    "\n",
    "And in this fake table, you might notice the probability\n",
    "of _Virginia Opossum_ is about the same in urban and rural,\n",
    "but the _Nine-banded Armadillo_ is 2.4 times more likely\n",
    "in rural than urban environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### (30%) Perform pattern mining with the `mlxtend` library \n",
    "\n",
    "Now that we have data, let's perform additional\n",
    "\n",
    " analysis on it.\n",
    "\n",
    "In the prevous part we use the _proximity_ of forest density as a\n",
    "proxy for _urbanization_.  While this may not fully be\n",
    "a founded assumption, for the purpose of this \n",
    "assignment that assumption is sufficient.\n",
    "\n",
    "One interesting area to explore, is which \n",
    "species are out _earlier_ in the evening,\n",
    "versus _later_ and whether the setting (rural or \n",
    "urban) makes a difference.  Thus, we split our data\n",
    "into a quadrant: \n",
    "\n",
    "|  | early evening (_9p-1159p_) | late evening (_12a-4a_) |\n",
    "| --:    | :-:| :-:| \n",
    "| urban |  ?  | ?  | \n",
    "| rural |  ?  | ?  |\n",
    "\n",
    "You might start to wonder a number of things\n",
    "and already be thinking maybe you have the data\n",
    "to answer this or that question.  And you might be right\n",
    "but ... we are going to throw a twist into our \n",
    "inquiry.  We are going to consider this a\n",
    "_frequent pattern mining_ problem and \n",
    "think about this using tools for mining\n",
    "frequent patterns.\n",
    "\n",
    "If we can convert the data into a binary\n",
    "database (binary table, binarized matrix, etc.)\n",
    "we can easily learn which species occur\n",
    "frequently with one another. To researchers\n",
    "in this area, this might be a very valuable \n",
    "thing to know.\n",
    "\n",
    "The setup goes something like this:\n",
    "\n",
    "* if every evening was grouped \n",
    "such that the rows were the dates\n",
    "and the columns the species, we\n",
    "could consider it a transaction\n",
    "$t \\in T$ with items \n",
    "$i \\in I$, where\n",
    "$i$ are just the species occuring\n",
    "in that date (transaction $t$)\n",
    "\n",
    "* once we have such a representation\n",
    "it is a trivial matter to use\n",
    "the libraries designed to do these\n",
    "types of pattern mining\n",
    "\n",
    "You will (minimally) need to study the\n",
    "frequent patterns functions in \n",
    "[mlxtend](https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/) to complete this task:\n",
    "\n",
    "* [`apriori()`](https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)\n",
    "\n",
    "You will also need to execute to install seaborn:\n",
    "\n",
    "```bash\n",
    "  !conda install -y mlxtend -c conda-forge # you must use -y \n",
    "```\n",
    "\n",
    "Once we have everything set up,\n",
    "we would ultimately be able to answer\n",
    "the questions below. Remember each question \n",
    "**must be accompanied by the corresponding\n",
    "Pandas and mlxtend code to earn full credit**:\n",
    "\n",
    "**&#167; Task:**  (_Frequent Pattern Mining_) \n",
    "\n",
    "Which _early evening_ species are frequent together in groups of 2 or more regardless\n",
    "of rural or urban? (Use a support of $0.50$ when answering).\n",
    "\n",
    "\n",
    "**&#167; Task:**  (_Frequent Pattern Mining_) \n",
    "\n",
    "Show the _early evening_ species frequent patterns for both urban and rural.\n",
    "Compare and contrast them.  Do you see any surprises or are these what you expect?\n",
    "\n",
    "\n",
    "**&#167; Task:**  (_Frequent Pattern Mining_) \n",
    "\n",
    "Show the _late evening_ species frequent patterns for both urban and rural.\n",
    "Compare and contrast them.  Do you see any surprises or are these what you expect?\n",
    "\n",
    "\n",
    "**&#167; Task:**  (_Reflection_) \n",
    "\n",
    "Fill in the table with the most frequent itemset (with length $> 1$) of highest support for each:\n",
    "\n",
    "|  | early evening (_9p-1159p_) | late evening (_12a-4a_) |\n",
    "| --:   | :-: | :-: | \n",
    "| urban |  (A, B); $\\mathit{sup} = 0.45$ |  (A, B, G); $\\mathit{sup} = 0.45$ | \n",
    "| rural |   (A, H, K, N); $\\mathit{sup} = 0.45$  | (C, B); $\\mathit{sup} = 0.65$  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sau-fall-2023]",
   "language": "python",
   "name": "conda-env-sau-fall-2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
