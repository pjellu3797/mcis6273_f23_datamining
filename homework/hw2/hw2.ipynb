{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "\\begin{huge}\n",
    "MCIS6273 Data Mining (Prof. Maull) / Fall 2023 / HW2\n",
    "\\end{huge}\n",
    "\\end{center}\n",
    "\n",
    "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n",
    "|:---------------:|:--------:|:---------------:|\n",
    "| 40 | Sunday, December 10 @ Midnight | _up to_ 24 hours |\n",
    "\n",
    "\n",
    "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n",
    "\n",
    "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n",
    "\n",
    "## OBJECTIVES\n",
    "* Perform data engineering on assignment dataset.\n",
    "\n",
    "* [supervised learning] Perform K-means analysis on real-world data.\n",
    "\n",
    "* [supervised learning/advances] Listen to this podcast about the future of search and advances in supervised learning.\n",
    "\n",
    "## WHAT TO TURN IN\n",
    "You are being encouraged to turn the assignment in using the provided\n",
    "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n",
    "`homework/hw2`.   Put all of your files in that directory.  Then zip that directory,\n",
    "rename it with your name as the first part of the filename (e.g. `maull_hw2_files.zip`), then\n",
    "download it to your local machine, then upload the `.zip` to Blackboard.\n",
    "\n",
    "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n",
    "on the basics of using zip in Linux.\n",
    "\n",
    "If you choose not to use the provided notebook, you will still need to turn in a\n",
    "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n",
    "this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ASSIGNMENT TASKS\n",
    "### (30%) Perform data engineering on assignment dataset. \n",
    "\n",
    "In this part of the assignment we will be introduced\n",
    "to a specific dataset that will be very interesting\n",
    "and unique.\n",
    "\n",
    "In the US (and most other counties), regulatory agencies\n",
    "control and inspect food and other products that come\n",
    "into the country and or otherwise imported for sale\n",
    "by wholesalers and retailers.\n",
    "\n",
    "One such area that we are going to explore is in what\n",
    "is called \"import refusal\".  Import refusal refers to\n",
    "the regulary mechanism which rejects products from\n",
    "importation into the coutry by way of the US FDA \n",
    "(Food and Drug Administration) --\n",
    "specificially if an inspected regulated product\n",
    "is not in compliance with FDA standards, the \n",
    "owner/cosignee is allowed to respond to the refusal\n",
    "and either show evidence that the product is \n",
    "in compliance (i.e. the FDA made a mistake) or\n",
    "produce a plan to bring the product into \n",
    "compliance -- otherwise the product is exported\n",
    "back to the owner/cosignee or destroyed.\n",
    "\n",
    "The mechanism used to track these refusals is\n",
    "called the IRR or \"Import Refusal Report\".\n",
    "\n",
    "You can (and might want to) read more about this IRR here:\n",
    "\n",
    "* [https://www.fda.gov/industry/fda-import-process/import-refusals](https://www.fda.gov/industry/fda-import-process/import-refusals)\n",
    "\n",
    "We will explore the data in this report with\n",
    "some unsupervised learning mechanisms, but\n",
    "before we do, will do perform some standard\n",
    "data engineering to bring the report into alignment\n",
    "with the tools required to do what we'd like later.\n",
    "\n",
    "**&#167; Task:**  Load and unzip the compressed ZIP import refusal report for 2014-present.  You can use\n",
    "Jupyter \"magics\" (the easiest way), or you can write Python code to load\n",
    "the ZIP file, and unzip it.  If you choose that method, you may like \n",
    "to make use of:\n",
    "\n",
    "* [Python Requests library](https://docs.python-requests.org/en/master/index.html)\n",
    "* [Python zip/unzip `zipfile` library](https://docs.python.org/3/library/zipfile.html)\n",
    "\n",
    "You will find the file on this page: \n",
    "\n",
    "* [https://www.accessdata.fda.gov/scripts/ImportRefusals/index.cfm](https://www.accessdata.fda.gov/scripts/ImportRefusals/index.cfm)\n",
    "\n",
    "And to get the URL of the ZIP file, select the 2014-present file and open your browser developer \n",
    "tools, click Download and see the actual URL (watch network traffic tab), or (less fun), use this URL:\n",
    "\n",
    "* [2014-present.zip](https://www.accessdata.fda.gov/scripts/ImportRefusals/downloads/Import_Refusal_2014-present.zip) \n",
    "\n",
    "\n",
    "**&#167; Task:**  Now that you have the data, you will read the CSV file.  Produced a \n",
    "file called `\"country_violations_2014-2023.csv\"` which contains just the\n",
    "counts of the violations **grouped by** `['ISO_CNTRY_CODE','PROVINCE_STATE','CITY_NAME']`.\n",
    "\n",
    "* you will need to use `groupby().count()` and restrict the columns to a single \n",
    "  column (`ENTRY_NUM` will do) using `.loc()`\n",
    "\n",
    "\n",
    "**&#167; Task:**  Produce a CSV file which includes the same data as `\"country_violations_2014-2023.csv\"`\n",
    "except it groups by `'YEAR'`, `'MONTH'`, `'ISO_CNTRY_CODE','PROVINCE_STATE','CITY_NAME'`.  You will\n",
    "might like to take the column `REFUSAL_DATE` and break it into a `YEAR` and `MONTH`\n",
    "column of its own, then do the grouping.  You new file should be called \n",
    "`\"country_violations_year_month_2014-2023.csv\"`.\n",
    "\n",
    "\n",
    "**&#167; Task:**  Which city, country, province had the most violations in a single month?  How many?  Which month and year?\n",
    "\n",
    "\n",
    "**&#167; Task:**  What are the 10 most frequent products in the IRR for 2018 (using `'PRDCT_CODE_DESC_TEXT'`)?\n",
    "\n",
    "\n",
    "**&#167; Task:**  **BONUS (+1 point)** What was the company associated with the largest violation in a single month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/envs/sau-fall-2023/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/sau-fall-2023/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/sau-fall-2023/lib/python3.11/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/sau-fall-2023/lib/python3.11/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/sau-fall-2023/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file extracted to /home/jovyan/homework/mcis6273_f23_datamining/homework/hw2\n",
      "Data grouped and saved to country_violations_2014-2023.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1026/1495859654.py:53: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['REFUSAL_DATE'] = pd.to_datetime(df['REFUSAL_DATE'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data grouped by year and month and saved to country_violations_year_month_2014-2023.csv\n",
      "City with the most violations: Ciudad De Mexico\n",
      "Country: MX\n",
      "Province/State: Ciudad de Mexico\n",
      "Number of violations: 613\n",
      "Month: 4\n",
      "Year: 2022\n",
      "Top 10 most frequent products in IRR for 2018:\n",
      "PRDCT_CODE_DESC_TEXT\n",
      "SUNGLASSES (NON-PRESCRIPTION INCLUDING PHOTOSENSITIVE)                                               575\n",
      "MISCELLANEOUS PATENT MEDICINES, ETC.                                                                 433\n",
      "VITAMIN, MINERAL, PROTEINS AND UNCONVENTIONAL DIETARY SPECIALITIES FOR HUMANS AND ANIMALS, N.E.C.    298\n",
      "TUNA (ALBACORE, YELLOWFIN, BLUEFIN, SKIPJACK, ETC.)                                                  244\n",
      "HERBALS & BOTANICALS (NOT TEAS), N.E.C.                                                              237\n",
      "MAHI MAHI                                                                                            190\n",
      "ULTRAVIOLET SCREEN/SUNSCREEN N.E.C.                                                                  183\n",
      "GLOVE, PATIENT EXAMINATION, POLY                                                                     173\n",
      "BATH SOAPS AND DETERGENTS (NOT ANTIPERSPIRANT) (PERSONAL CLEANLINESS)                                172\n",
      "LENSES, SOFT CONTACT, DAILY WEAR                                                                     154\n",
      "Name: count, dtype: int64\n",
      "Company associated with the largest violation in a single month: COMERCIALIZADORA PEPSICO\n"
     ]
    }
   ],
   "source": [
    "# Load and unzip the compressed ZIP import refusal report for 2014-present\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "zip_url = \"https://www.accessdata.fda.gov/scripts/ImportRefusals/downloads/Import_Refusal_2014-present.zip\"\n",
    "\n",
    "# Send a GET request to the URL to download the ZIP file\n",
    "response = requests.get(zip_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Create a file-like object from the response content\n",
    "    zip_data = io.BytesIO(response.content)\n",
    "\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_data, 'r') as zip_ref:\n",
    "        # Get the directory where the Python script is located\n",
    "        script_directory = os.getcwd()\n",
    "        zip_ref.extractall(script_directory)\n",
    "        print(f\"ZIP file extracted to {script_directory}\")\n",
    "else:\n",
    "    print(\"Failed to download the ZIP file\")\n",
    "    \n",
    "\n",
    "# Now that you have the data, you will read the CSV file. Produced a file called \"country_violations_2014-2023.csv\" which contains just the counts of the violations grouped by ['ISO_CNTRY_CODE','PROVINCE_STATE','CITY_NAME'].\n",
    "# Load the CSV file\n",
    "csv_file_path = \"REFUSAL_ENTRY_2014-October2023.csv\"\n",
    "encodings_to_try = ['utf-8', 'latin-1', 'ISO-8859-1']\n",
    "\n",
    "# Attempt to read the file with different encodings\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path, encoding=encoding)\n",
    "        break  # If successful, exit the loop\n",
    "    except UnicodeDecodeError:\n",
    "        continue  # Try the next encoding if decoding fails\n",
    "\n",
    "# Group by ['ISO_CNTRY_CODE', 'PROVINCE_STATE', 'CITY_NAME'] and count violations\n",
    "grouped_df = df.groupby(['ISO_CNTRY_CODE', 'PROVINCE_STATE', 'CITY_NAME'])['ENTRY_NUM'].count().reset_index()\n",
    "\n",
    "# Save the grouped DataFrame to a new CSV file\n",
    "output_csv_file = \"country_violations_2014-2023.csv\"\n",
    "grouped_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(f\"Data grouped and saved to {output_csv_file}\")\n",
    "\n",
    "\n",
    "# Produce a CSV file which includes the same data as \"country_violations_2014-2023.csv\" except it groups by 'YEAR', 'MONTH', 'ISO_CNTRY_CODE','PROVINCE_STATE','CITY_NAME'. You will might like to take the column REFUSAL_DATE and break it into a YEAR and MONTH column of its own, then do the grouping. You new file should be called \"country_violations_year_month_2014-2023.csv\".\n",
    "# Extract 'YEAR' and 'MONTH' from 'REFUSAL_DATE' column\n",
    "df['REFUSAL_DATE'] = pd.to_datetime(df['REFUSAL_DATE'])\n",
    "df['YEAR'] = df['REFUSAL_DATE'].dt.year\n",
    "df['MONTH'] = df['REFUSAL_DATE'].dt.month\n",
    "\n",
    "# Group by 'YEAR', 'MONTH', 'ISO_CNTRY_CODE', 'PROVINCE_STATE', and 'CITY_NAME' and count violations\n",
    "grouped_month_year_df = df.groupby(['YEAR', 'MONTH', 'ISO_CNTRY_CODE', 'PROVINCE_STATE', 'CITY_NAME'])['ENTRY_NUM'].count().reset_index()\n",
    "\n",
    "# Save the grouped DataFrame to a new CSV file\n",
    "month_year_output_csv_file = \"country_violations_year_month_2014-2023.csv\"\n",
    "grouped_month_year_df.to_csv(month_year_output_csv_file, index=False)\n",
    "\n",
    "print(f\"Data grouped by year and month and saved to {month_year_output_csv_file}\")\n",
    "\n",
    "\n",
    "# Which city, country, province had the most violations in a single month? How many? Which month and year?\n",
    "# Load the grouped data CSV file\n",
    "grouped_month_year_csv_file = \"country_violations_year_month_2014-2023.csv\"  # Replace with the actual file path\n",
    "df1 = pd.read_csv(grouped_month_year_csv_file)\n",
    "\n",
    "# Find the row with the maximum number of violations\n",
    "max_violations_row = df1[df1['ENTRY_NUM'] == df1['ENTRY_NUM'].max()]\n",
    "\n",
    "# Print the results\n",
    "print(f\"City with the most violations: {max_violations_row['CITY_NAME'].iloc[0]}\")\n",
    "print(f\"Country: {max_violations_row['ISO_CNTRY_CODE'].iloc[0]}\")\n",
    "print(f\"Province/State: {max_violations_row['PROVINCE_STATE'].iloc[0]}\")\n",
    "print(f\"Number of violations: {max_violations_row['ENTRY_NUM'].iloc[0]}\")\n",
    "print(f\"Month: {max_violations_row['MONTH'].iloc[0]}\")\n",
    "print(f\"Year: {max_violations_row['YEAR'].iloc[0]}\")\n",
    "\n",
    "# What are the 10 most frequent products in the IRR for 2018 (using 'PRDCT_CODE_DESC_TEXT')?\n",
    "# Convert 'REFUSAL_DATE' to datetime\n",
    "df['REFUSAL_DATE'] = pd.to_datetime(df['REFUSAL_DATE'])\n",
    "\n",
    "# Filter the data for the year 2018\n",
    "df_2018 = df[df['REFUSAL_DATE'].dt.year == 2018]\n",
    "\n",
    "# Find the 10 most frequent products in 2018\n",
    "top_10_products_2018 = df_2018['PRDCT_CODE_DESC_TEXT'].value_counts().head(10)\n",
    "\n",
    "# Print the result\n",
    "print(\"Top 10 most frequent products in IRR for 2018:\")\n",
    "print(top_10_products_2018)\n",
    "\n",
    "\n",
    "# What was the company associated with the largest violation in a single month?\n",
    "# Group by 'YEAR', 'MONTH', 'LGL_NAME', and 'ENTRY_NUM' and count violations\n",
    "company_violation_df = df.groupby(['YEAR', 'MONTH', 'LGL_NAME'])['ENTRY_NUM'].count().reset_index()\n",
    "\n",
    "# Find the maximum violation in a single month\n",
    "max_violation = company_violation_df['ENTRY_NUM'].max()\n",
    "\n",
    "# Find the associated company with the maximum violation\n",
    "company_with_max_violation = company_violation_df[company_violation_df['ENTRY_NUM'] == max_violation]['LGL_NAME'].iloc[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Company associated with the largest violation in a single month: {company_with_max_violation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### (50%) [supervised learning] Perform K-means analysis on real-world data. \n",
    "\n",
    "The goal of cluster analysis is to extract data patterns \n",
    "from data that does not contain  (or is not used for) training\n",
    "instances.  We call this _unsupervised learning_, since there\n",
    "is no training data to build models from.\n",
    "\n",
    "Instead, we use some of the commonly studied _distance metrics_\n",
    "to develop a notion of similarity.  Indeed, we are trying to\n",
    "optimize for instances of a cluster to maximize _intra_-cluster \n",
    "(within-cluster) similarity, while _inter_-cluster similarity is\n",
    "minimized -- put another way, instances that belong to a cluster\n",
    "should look close to one another.\n",
    "\n",
    "There are many clustering algorithms, but one of the most robust and\n",
    "useful is the $K$-means algorithm.  \n",
    "\n",
    "**&#167; Task:**  Prepare the data such that you have three datasets where the `REFUSAL_CHARGES`,\n",
    "`ISO_CNTRY_CODE` and `CITY_NAME` are the columns (features).\n",
    "\n",
    "You will need to use the `LabelBinarizer()` (for countries and cities) and `MultiLabelBinarizer()` (for the charges)\n",
    "of the sklearn libraries.\n",
    "\n",
    "See:\n",
    "\n",
    "* [`sklearn.preprocessing.LabelBinarizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) \n",
    "* [`sklearn.preprocessing.MultiLabelBinarizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html)\n",
    "\n",
    "\n",
    "**&#167; Task:**  Merge the three Dataframes above into one -- the final Dataframe should have 435 columns.  These\n",
    "represent the features that will allow clustering to occur.  This will allow us to see\n",
    "the clusters that emerge along those categories of features in the data.  With a bit \n",
    "more information, we might come to the conclusion that some of these features should\n",
    "be removed.\n",
    "\n",
    "\n",
    "**&#167; Task:**  You will now take the dataset from the first part and begin the process of \n",
    "clustering.\n",
    "\n",
    "To be successful, please study the following:\n",
    "\n",
    "* [K-Means in scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "* [K-Means example notebook](https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb)\n",
    "\n",
    "You will set three $K$ to 5, 10 and 12.  You will need to report the centroids\n",
    "for each cluster and in words how you would describe that cluster.  I will give more guidance \n",
    "on this.\n",
    "\n",
    "\n",
    "**&#167; Task:**  _(Perform elbow analysis to find optimal cluster size)_\n",
    "\n",
    "In the previous part, we chose the cluster size $K$.  Another way to do this \n",
    "is to analyze the change in within cluster sum of squares and \n",
    "determine when such value fails to change significantly.  In other words,\n",
    "when the addition of another cluster fails to significantly change\n",
    "the within cluster sum of squares, then you can be confident \n",
    "more clusters won't make a difference (increasing $K$ will no longer be \n",
    "relevant).\n",
    "\n",
    "This is often referred to as \"Elbow Analysis\" or the \"Elbow Method\" because \n",
    "you will visually find the elbow in a plot of the sum of squares \n",
    "and choose $K$ based on that. \n",
    "\n",
    "Study the following code, implement it, and find the optimal $K$ based on it.\n",
    "\n",
    "Your answer must include:\n",
    "\n",
    "* the elbow graph\n",
    "* the optimal $K$\n",
    "* the reanalysis of the previous answer based on the optimal $K$ (re-run your clusters and report their centroid characteristics)\n",
    "\n",
    "Here is the code to help you:\n",
    "\n",
    "```python\n",
    "max_clusters = 15\n",
    "css = [] # within cluster sum of squares\n",
    "\n",
    "for k in range(1,max_clusters):\n",
    "  kmeans = KMeans(n_clusters=k, 'k-means++', max_iter=200, n_init=10, random_state=0)\n",
    "  kmeans.fit(d) # where d is the dataset you have standardized in the first part of this\n",
    "  css.append(kmeans.inertia_)\n",
    "\n",
    "# now make a line plot of all the values in css     \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (20%) [supervised learning/advances] Listen to this podcast about the future of search and advances in supervised learning. \n",
    "\n",
    "We are, as you know, entering an entirely new era of \n",
    "human-machine interaction.\n",
    "\n",
    "LLMs, popularized by ChatGPT, Bard and others, are pushing new\n",
    "paradigms of interaction with machines, fueling what many are\n",
    "calling a breakthrough in Artificial Intelligence unlike\n",
    "any seen in prior advances in the field (AI, has after all, \n",
    "been the intense study of computer scientists since the\n",
    "1950s).\n",
    "\n",
    "One obvious area where this is going to be immediately obvious \n",
    "is in search.  For some time, we have been using search engines\n",
    "with the \"poke and hope\" method -- typing in some keywords\n",
    "and hoping we get what we are looking for.  Have you noticed\n",
    "that this method doesn't work that well?  Have you also noticed\n",
    "that sometimes you spend more time trying to find the \"right\"\n",
    "keywords instead of getting to what you want with just\n",
    "the words you have to express what you reall mean?\n",
    "\n",
    "One reason for this is that our mental models for what \n",
    "we are searching for are incomplete -- we often don't \n",
    "have enough domain knowledge to phrase the question\n",
    "in a way that would yield answers even remotely\n",
    "close to what we would like ...\n",
    "\n",
    "That is all about to change.\n",
    "\n",
    "You will listen to this 59 minute podcast interview \n",
    "with the CEO of   \n",
    "Perplexity AI -- a company focused on using AI \n",
    "to improve learning.  Search might be the first test\n",
    "case to demonstrate what is coming in many other areas\n",
    "of the interesting use cases of AI.\n",
    "\n",
    "Listen to this podcast:\n",
    "\n",
    "* Machine Learning Street Talk (MLST): _Perplexity AI_: The Future of Search; May 8, 2023; Interview with Aravind Srinivas, \n",
    "  CEO and co founder of [Perplexity AI](https://www.perplexity.ai/).  You will find a variety\n",
    "  of sources of the interview (pick one):\n",
    "\n",
    "  * [Apple Podcasts](https://podcasts.apple.com/us/podcast/perplexity-ai-the-future-of-search/id1510472996?i=1000612223005)\n",
    "  * [Player.fm](https://player.fm/series/machine-learning-street-talk-mlst/perplexity-ai-the-future-of-search)\n",
    "  * [Spotify](https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/PERPLEXITY-AI%C3%A2%E2%82%AC%E2%80%9DThe-future-of-search-e23i5fq)\n",
    "  * [Youtube (**no ads**, no tracking, viewed through DDG)](https://duckduckgo.com/?q=machine+learning+street+talk+perplexity&t=vivaldi&iax=videos&ia=videos&iai=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D_vMOWw3uYvk)\n",
    "  * [Youtube direct (ads, account tracking, etc.)](https://www.youtube.com/watch?v=_vMOWw3uYvk)\n",
    "\n",
    "Find out what all the hullabuloo is about.\n",
    "\n",
    "**&#167; Task:**  Summarize the 3-5 main takeaways of the interview?  Be brief. \n",
    "\n",
    "\n",
    "**&#167; Task:**  Provide 3-5 sentences expressing your reactions to the interview?  Be direct, succinct and precise. \n",
    "\n",
    "\n",
    "**&#167; Task:**  What _one thing_ did you find most interesting or surprising in the interview? \n",
    "\n",
    "\n",
    "**&#167; Task:**  Provide _one_ criticism or concern of the work of Perplexity AI, and expand on that criticism with\n",
    "a few sentences explaining why you feel your criticism/concern is warranted."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sau-fall-2023]",
   "language": "python",
   "name": "conda-env-sau-fall-2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
