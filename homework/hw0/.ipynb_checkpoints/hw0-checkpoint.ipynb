{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "\\begin{huge}\n",
    "MCIS6273 Data Mining (Prof. Maull) / Fall 2023 / HW0\n",
    "\\end{huge}\n",
    "\\end{center}\n",
    "\n",
    "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n",
    "|:---------------:|:--------:|:---------------:|\n",
    "| 20 | Tuesday September 26 @ Midnight | _up to_ 4 hours |\n",
    "\n",
    "\n",
    "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n",
    "\n",
    "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n",
    "\n",
    "## OBJECTIVES\n",
    "* Familiarize yourself with Github and basic git\n",
    "\n",
    "* Familiarize yourself with the JupyterLab environment, Markdown and Python\n",
    "\n",
    "* Explore JupyterHub Linux console integrating what you learned in the prior parts of this homework\n",
    "\n",
    "* Listen to the Talk Python To Me from July 7, 2023: How data scientists use Python\n",
    "\n",
    "* Perfom basic data engineering in Python using Gutenberg.org text of Bertrand Russell's 1912 work _The Problems of Philosophy_\n",
    "\n",
    "* Use structured data to develop basic statistical analyses\n",
    "\n",
    "## WHAT TO TURN IN\n",
    "You are being encouraged to turn the assignment in using the provided\n",
    "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n",
    "`homework/hw0`.   Put all of your files in that directory.  Then zip that directory,\n",
    "rename it with your name as the first part of the filename (e.g. `maull_hw0_files.zip`), then\n",
    "download it to your local machine, then upload the `.zip` to Blackboard.\n",
    "\n",
    "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n",
    "on the basics of using zip in Linux.\n",
    "\n",
    "If you choose not to use the provided notebook, you will still need to turn in a\n",
    "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n",
    "this homework.\n",
    "\n",
    "\n",
    "## ASSIGNMENT TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0%) Familiarize yourself with Github and basic git \n",
    "\n",
    "[Github (https://github.com)](https://github.com) is the _de facto_ platform for open source software in the world based\n",
    "on the very popular [git (https://git-scm.org)](https://git-scm.org) version control system. Git has a sophisticated set\n",
    "of tools for version control based on the concept of local repositories for fast commits and remote\n",
    "repositories only when collaboration and remote synchronization is necessary.  Github enhances git by providing\n",
    "tools and online hosting of public and private repositories to encourage and promote sharing and collaboration.\n",
    "Github hosts some of the world's most widely used open source software.\n",
    "\n",
    "**If you are already familiar with git and Github, then this part will be very easy!**\n",
    "\n",
    "**&#167; Task:**  **Create a public Github repo named `\"mcis6273-f23-datamining\"` and place a readme.md file in it.**\n",
    "Create your first file called\n",
    "`README.md` at the top level of the repository.  \n",
    "\n",
    "Please put your Zotero username in the file. Aside from that you can put whatever text you like in the file \n",
    "(If you like, use something like [lorem ipsum](https://lipsum.com/)\n",
    "to generate random sentences to place in the file.).\n",
    "Please include the link to **your** Github repository that now includes the minimal `README.md`. \n",
    "You don't have to have anything elaborate in that file or the repo. \n",
    "\n",
    "\n",
    "**&#167; Task:**  Fork the course repository:\n",
    "\n",
    "* [https://github.com/kmsaumcis/mcis6273_f23_datamining/](https://github.com/kmsaumcis/mcis6273_f23_datamining/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10%) Familiarize yourself with the JupyterLab environment, Markdown and Python \n",
    "\n",
    "As stated in the course announcement [Jupyter (https://jupyter.org)](https://jupyter.org) is the\n",
    "core platform we will be using in this course and\n",
    "is a popular platform for data scientists around the world.  We have a JupyterLab\n",
    "setup for this course so that we can operate in a cloud-hosted environment, free from\n",
    "some of the resource constraints of running Jupyter on your local machine (though you are free to set\n",
    "it up on your own and seek my advice if you desire).\n",
    "\n",
    "You have been given the information about the  Jupyter environment we have setup for our course, and\n",
    "the underlying Python environment will be using is the [Anaconda (https://anaconda.com)](https://anaconda.com)\n",
    "distribution.  It is not necessary for this assignment, but you are free to look at the multitude\n",
    "of packages installed with Anaconda, though we will not use the majority of them explicitly.\n",
    "\n",
    "As you will soon find out, Notebooks are an incredibly effective way to mix code with narrative\n",
    "and you can create cells that are entirely code or entirely Markdown.  Markdown (MD or `md`) is\n",
    "a highly readable text format that allows for easy documentation of text files, while allowing\n",
    "for HTML-based rendering of the text in a way that is style-independent.\n",
    "\n",
    "We will be using Markdown frequently in this course, and you will learn that there are many different\n",
    "\"flavors\" or Markdown.  We will only be using the basic flavor, but you will benefit from exploring\n",
    "the \"Github flavored\" Markdown, though you will not be responsible for using it in this course -- only the\n",
    "\"basic\" flavor.  Please refer to the original course announcement about Markdown.\n",
    "\n",
    "**&#167; Task:**  **THERE IS NOTHING TO TURN IN FOR THIS PART.** \n",
    "\n",
    "Play with and become familiar with the basic functions of\n",
    "the Lab environment given to you online in the course Blackboard.\n",
    "\n",
    "\n",
    "**&#167; Task:**  **Please _create a markdown document_ called `semester_goals.md` with 3 sentences/fragments that\n",
    "answer the following question:**\n",
    "\n",
    "* **What do you wish to accomplish this semester in Data Mining?**\n",
    "\n",
    "Read the documentation for basic Markdown [here](https://www.markdownguide.org/basic-syntax). \n",
    "Turn in the text `.md` file *not* the processed `.html`.  In whatever you turn in, \n",
    "you must show the use of *ALL* the following:\n",
    "\n",
    "* headings (one level is fine),\n",
    "* bullets,\n",
    "* bold and italics\n",
    "\n",
    "Again, the content of your document needs to address the question above and it should live\n",
    "in the top level directory of your assignment submission.  This part will be graded but no\n",
    "points are awarded for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0%) Explore JupyterHub Linux console integrating what you learned in the prior parts of this homework \n",
    "\n",
    "The Linux console in JupyterLab is a great way to perform command-line tasks and is an essential tool\n",
    "for basic scripting that is part of a data scientist's toolkit.  Open a console in the lab environment\n",
    "and familiarize yourself with your files and basic commands using git as indicated below.\n",
    "\n",
    "1. In a new JupyterLab command line console, run the `git clone` command to clone the new\n",
    "  repository you created in the prior part.\n",
    "  You will want to read the documentation on this \n",
    "  command (try here [https://www.git-scm.com/docs/git-clone](https://www.git-scm.com/docs/git-clone) to get a good\n",
    "  start).\n",
    "2. Within the same console, modify your `README.md` file, check it in and push it back to your repository, using\n",
    "  `git push`.  Read the [documentation about `git push`](https://git-scm.com/docs/git-push).\n",
    "3. The commands `wget` and `curl` are useful for grabbing data and files from remote resources off the web.\n",
    "  Read the documentation on each of these commands by typing `man wget` or `man curl` in the terminal.\n",
    "  Make sure you pipe the output to a file or use the proper flags to do so.\n",
    "\n",
    "**&#167; Task:**  **THERE IS NOTHING TO TURN IN FOR THIS PART.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (30%) Listen to the Talk Python To Me from July 7, 2023: How data scientists use Python \n",
    "\n",
    "Data science is one of the most important and \"hot\" disciplines today\n",
    "and there is a lot going on from data engineering to modeling and\n",
    "analysis. Python is critial to the data scientists \n",
    "toolkit, but they are interesting in their own right.  \n",
    "\n",
    "Why?\n",
    "\n",
    "In this short, interesting and informative podcast, you will learn about\n",
    "the reasons why Python is so hot, and how Python made it to the top \n",
    "of the data science stack.\n",
    "\n",
    "Please listen to this one hour podcast and answer some of the \n",
    "questions below. You can listen to it from one of the two links below:\n",
    "\n",
    "* Talk Python['Podcast'] [Show #433: How data scientists use Python](https://talkpython.fm/episodes/show/422/how-data-scientists-use-python)\n",
    "* direct link to mp3 file [how-data-scientists-use-python.mp3](https://talkpython.fm/episodes/download/422/how-data-scientists-use-python.mp3)\n",
    "\n",
    "**&#167; Task:**  **PLEASE ANSWER THE FOLLOWING QUESTIONS AFTER LISTENING TO THE PODCAST**:\n",
    "\n",
    "  1. List 3 things that you learned from this podcast?\n",
    "  2. What is your reaction to the podcast? Pick at least one point brought up in the interview that you agree with and list your reason why.\n",
    "  3. After listening to the podcast, do you think you are more informed about the importance of Python to Data Science?  How? (Be brief -- one sentence will suffice.)\n",
    "  4. List one _surprising_ fact you learned from listening to this podcast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "1. How BI analyst differs from Data Scientist. Data Scientist spend lot of time in gathering data. Importance of Python libraries like pandas, NumPy and PySpark.\r\n",
    "2. My reaction to the podcast was highly positive. I believe that data scientists should collaborate with the development team from the project's inception rather than joining later stages. This approach ensures they have a comprehensive understanding of the project's progress and requirements, promoting effective teamwork and alignment throughout the project's lifecycle.\r\n",
    "3. Yes, After listening to the podcast, I am more informed about Python's significance in Data Science due to its powerful prototyping capabilities and extensive libraries supporting data mining.\r\n",
    "4. One surprising fact I learned from the podcast is that Pandas 2.0 is now available, offering significantly improved speed compared to its predecessor, although it still has some areas where it lac\n",
    "ks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (30%) Perfom basic data engineering in Python using Gutenberg.org text of Bertrand Russell's 1912 work _The Problems of Philosophy_ \n",
    "\n",
    "You learned from the prior part that data science\n",
    "is one of Python's strengths.\n",
    "\n",
    "In this part, you will interact directly with those\n",
    "strengths, but in a way that will allow you to see the \n",
    "challenges that you will face and confront as a real-world\n",
    "data scientist.\n",
    "\n",
    "_Data engineering_ as you have learned from the readings\n",
    "is about transforming data from one form to another so\n",
    "that it can be used in the appropriate analysis \n",
    "contexts.\n",
    "\n",
    "One area of intense work is in transforming unstructured\n",
    "data, like a book or text, into structured data.  More \n",
    "importantly, producing statistical analyses of these \n",
    "unstructured data is often difficult, because one\n",
    "must convert that unstructured data to something that\n",
    "a machine can process algorithmically.\n",
    "\n",
    "In this part of the homework you will take a text \n",
    "from the Project Gutenberg [https://gutenberg.org](https://gutenberg.org)\n",
    "and convert it to something more structured.  In fact,\n",
    "you will convert it to multiple structured forms.\n",
    "\n",
    "For this part we will be working with Betrand Russell's 1912 work _The Problems of Philosphy_\n",
    "which is located at the Project Gutenberg's website [https://gutenberg.org](https://gutenberg.org).\n",
    "The `.txt` file you will want to work with is here:\n",
    "\n",
    "* [https://www.gutenberg.org/cache/epub/5827/pg5827.txt](https://www.gutenberg.org/cache/epub/5827/pg5827.txt)\n",
    "\n",
    "If you are not familiar with Betrand Russell, \n",
    "you may want to be.  He is widely regarded as an \n",
    "important and influential 20th century western logician, mathematician and\n",
    "philosopher who made prolific, deep and crucial contributions to the\n",
    "philosophy of mathematics, logic, set theory, computer science,\n",
    "artificial intelligence, epistemology and metaphysics. \n",
    "\n",
    "Additionally, if you are unfamiliar with Project Gutenberg, you can learn more about it\n",
    "here: [https://gutenberg.org/about/background/](https://gutenberg.org/about/background/). It \n",
    "is an essential repository of many classic books and \n",
    "texts which are now out of copyright, but more importantly it's founder, Michael Hart, \n",
    "invented eBooks\n",
    "in 1971, before probably all of us were born, and certainly before the widespread\n",
    "ubiquity of the public Internet as we know it.  It is a fascinating\n",
    "history that you should know a little about.\n",
    "\n",
    "For our purposes, though, what makes Gutenberg most interesting is that\n",
    "we can directly obtain the `.txt` version of the texts allowing us to \n",
    "use the power of Python to computationally process this unstructured data\n",
    "and convert it to something more useful to our machines and algorithms.\n",
    "\n",
    "Your code must be implemented in Jupyter as a notebook -- you\n",
    "will be required to turn in a `.ipynb` file.\n",
    "\n",
    "**&#167; Task:**  **Use Python to parse and tokenize the text file.**\n",
    "\n",
    "You will produce a `.csv` file which will have\n",
    "all the full words lowercase and\n",
    "with all punctuation removed _unless_ it is part\n",
    "of the word.  For example, if you have a token\n",
    "\"`world.`\", you will drop the ending period,\n",
    "however, if you have a word   \"`can't`\", you will\n",
    "retain the apostrophe \"`'`\".\n",
    "\n",
    "Your output `.csv` file will contain all the \n",
    "words in alphabetical order with their frequency \n",
    "counts.\n",
    "\n",
    "Here is an example of some lines in such a `.csv` file:\n",
    "\n",
    "```\n",
    "...\n",
    "\n",
    "the,112\n",
    "there,62\n",
    "thing,3\n",
    "this,200\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "**NOTE:** Only the words (first column) are sorted, the\n",
    "counts do not need to be sorted.\n",
    "\n",
    "Please name your file `all_words.csv`.\n",
    "\n",
    "\n",
    "**&#167; Task:**  Now that we have all the words, let's go back to the \n",
    "drawing board and **get all _capitalized_ (uppercase) words**.\n",
    "\n",
    "To do this, you will tokenize as before, but you will\n",
    "retain only those words that are capitalized.\n",
    "\n",
    "Also, as before, you will remove punctuation except\n",
    "when it is part of the word, such as an example\n",
    "of a possessive proper noun like \"`Carl's`\".\n",
    "\n",
    "You will also include the frequency counts of these\n",
    "capitalized words in _sorted_ order by word.\n",
    "\n",
    "Please name your file `all_uppercase_words.csv`\n",
    "\n",
    "\n",
    "**&#167; Task:**  **Answer the following questions:**\n",
    "\n",
    "  1. Which were the 5 most frequent words in `all_words.csv` were most frequent?\n",
    "  2. Which were the 5 most frequent words in `all_uppercase_words.csv`.\n",
    "  3. Compare and contrast these top 5.  Explain in 2-3 sentences what you observe about \n",
    "    the similariries and differences.\n",
    "  4. In your own words, what were the most surprising parts of each list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Use Python to parse and tokenize the text file.\n",
    "import string\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "file_path = 'pg5827.txt'\n",
    "output_file = 'all_words.csv'\n",
    "word_frequency = defaultdict(int)\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read the text from the file\n",
    "    text = file.read()\n",
    "    text = text.replace('“','')\n",
    "    text = text.replace('”','')\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        cleaned_word = word.strip(string.punctuation)\n",
    "        if cleaned_word:\n",
    "            cleaned_word = cleaned_word.lower()\n",
    "            word_frequency[cleaned_word] += 1\n",
    "\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Word', 'Frequency'])\n",
    "    for word in sorted(word_frequency.keys()):\n",
    "        csv_writer.writerow([word, word_frequency[word]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Task: Now that we have all the words, let's go back to the drawing board and get all capitalized (uppercase) words.\n",
    "\n",
    "import string\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "file_path = 'pg5827.txt'\n",
    "output_file = 'all_uppercase_words.csv'\n",
    "upper_word_frequency = defaultdict(int)\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read the text from the file\n",
    "    text = file.read()\n",
    "    text = text.replace('“','')\n",
    "    text = text.replace('”','')\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        cleaned_word = word.strip(string.punctuation)\n",
    "        if cleaned_word and cleaned_word[0].isupper():\n",
    "            upper_word_frequency[cleaned_word] += 1\n",
    "\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Word', 'Frequency'])\n",
    "    for word in sorted(upper_word_frequency.keys()):\n",
    "        csv_writer.writerow([word, upper_word_frequency [word]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "1. the, of, is, to, and\n",
    "2. The, I, It, But, Thus\n",
    "3. In both the cases we got words that we use commonly in english literature.  The top word is same in both the cases. The frequency of Capitals are way less than the all words frequency.\n",
    "4. I can see some junk words with some of the words.ist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (30%) Use structured data to develop basic statistical analyses \n",
    "\n",
    "Now that we have a sense of taking this text and producing\n",
    "some output files that are quite a bit more interesting,\n",
    "we are going to go further into some statistical \n",
    "analyses.\n",
    "\n",
    "Of course, one thing that we are concerned about in \n",
    "unstructured data, are elements that do not add much \n",
    "to our understanding or conversion of that data.\n",
    "\n",
    "One such area in the English language, at least (and most\n",
    "other languages), are words that do not increase the\n",
    "information of the sentence at an _essential_ level.\n",
    "\n",
    "For example, the word `'the'` is not a very useful word\n",
    "when analyzing text, and especially the words that add\n",
    "to the meaning of a sentence.  It is usually the \n",
    "_nouns_ and _verbs_ that get us to the useful parts,\n",
    "and then the _pronouns_, _adjectives_, _adverbs_, etc.\n",
    "Critically, the less common a word is, the more\n",
    "likely that word is important to understanding a text.\n",
    "\n",
    "We are going to delve into a basic and rudimentary \n",
    "statistical analysis of the text.\n",
    "\n",
    "When we are done, we should be able to answer a question\n",
    "like _How likely is it to see a sentence with the\n",
    "words `car`, `plant`, `simple`?_  We will also continue\n",
    "some basic data engineering along the way.\n",
    "\n",
    "**&#167; Task:**  **Remove the stopwords from your `all_words.csv` and put the \n",
    "remaining non-stopwords in a file `all_ns_words.csv`. Please \n",
    "retain the frequency column as before.**\n",
    "\n",
    "A good list of stopwords to start with can be found here:\n",
    "\n",
    "* [https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt](https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt)\n",
    "\n",
    "Furthermore, you can learn what a _stopword_ is from the excellent\n",
    "text Christopher D. Manning, Prabhakar Raghavan and \n",
    "Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008. \n",
    "[https://nlp.stanford.edu/IR-book/](https://nlp.stanford.edu/IR-book/).  :\n",
    "\n",
    "* here is primary source information on stopwords [https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)\n",
    "\n",
    "\n",
    "**&#167; Task:**  **Add a new column to your `all_ns_words.csv` that \n",
    "  contains the probability of that word.**\n",
    "\n",
    "To do this, use the denomator of the sum of stopwords\n",
    "**not** all words.  Alternatively, do not include\n",
    "stopword counts in your sum.\n",
    "\n",
    "Thus, $W$ are all words and if $w$ is a non-stopword, $w \\in W$, let $C_{w}$ be the\n",
    "frequency (count) of word $w$.  Thus, \n",
    "$$ \\Pr(w \\in W) = \\frac{C_w}{\\sum_{w' \\in W} C_{w'}}.$$\n",
    "\n",
    "Concretely, if \"`righteous`\" appears 200 times,\n",
    "and the sum of frequencies of all non-stopwords\n",
    "is 10000, then $\\Pr(w=righteous) = \\frac{200}{10000} = 0.02$.\n",
    "\n",
    "Your new file will look something like:\n",
    "\n",
    "```\n",
    "...\n",
    "friend, 112, .003\n",
    "fruit, 67, .00014\n",
    "grand, 88, .01763\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**&#167; Task:**  **Answer the following questions using your analysis and results from the text:**\n",
    "\n",
    "1. How many unique non-stop words are in the text?\n",
    "2. Which is the least probable word? (if there is a tie, please state the tie words)\n",
    "3. What observation can you make about the probabilities?\n",
    "4. Which sentence is more likely:\n",
    "\n",
    "    a. _If a belief is true, it can be deduced it is universal._\n",
    "    b. _Criticism of knowledge is counter to scientific results._\n",
    "    <br/>\n",
    "\n",
    "    You will use the sum of the probabilities of\n",
    "    each non-stop word to answer the question. You will need to \n",
    "    give numeric rationale for your answer. Show your work in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task: Remove the stopwords from your all_words.csv and put the remaining non-stopwords in a file all_ns_words.csv. Please retain the frequency column as before.\n",
    "import csv\n",
    "\n",
    "stopwords_file = 'stopwords-en.txt'\n",
    "input_csv = 'all_words.csv'\n",
    "output_csv = 'all_ns_words.csv'\n",
    "stopwords = []\n",
    "with open(stopwords_file, 'r') as file:\n",
    "    stopwords = set(line.strip() for line in file)\n",
    "\n",
    "word_frequency = {}\n",
    "with open(input_csv, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    next(csv_reader) \n",
    "    for row in csv_reader:\n",
    "        word = row[0]\n",
    "        frequency = int(row[1])\n",
    "        word_frequency[word] = frequency\n",
    "\n",
    "filtered_word_frequency = {word: frequency for word, frequency in word_frequency.items() if word.lower() not in stopwords}\n",
    "\n",
    "with open(output_csv, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Word', 'Frequency'])\n",
    "    for word, frequency in filtered_word_frequency.items():\n",
    "        csv_writer.writerow([word, frequency])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Add a new column to your all_ns_words.csv that contains the probability of that word.\n",
    "import csv\n",
    "\n",
    "csv_file = 'all_ns_words.csv'\n",
    "\n",
    "word_frequency = {}\n",
    "total_frequency = 0\r\n",
    "with open(csv_file, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        word_frequency[row[0]] = int(row[1])\n",
    "        total_frequency = total_frequency + int(row[1])\n",
    "\n",
    "\n",
    "with open(csv_file, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Word', 'Frequency', 'Probability'])\n",
    "    \n",
    "    for word, frequency in word_frequency.items():\n",
    "        probability = frequency / total_frequency\n",
    "        csv_writer.writerow([word, frequency, probability])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Answer the following questions using your analysis and results from the text:\n",
    "\n",
    "1. 2985\n",
    "2. 1.a, 1.b, 1.d, 1.e.2, 1.e.3, 1.e.4, 1.e.5, 1.e.6, 1.f, 1.f.1, 1.f.2, 1.f.4, 1.f.5, 1.f.6, 1500, 1596-1650, 1646-1716, 1685-1753, 1711-76, 1724-1804, 1770-1831, 1912, 20, 2001, 2004, 2019, 30, 5,000, 50, 501(c)(3, 5827, 596-1887, 60, 64-6221541, 801, 809, 84116, abide, abstain, abstracted, abstractions, abstractly, absurdities, acceptance, accepts, accessed, accessible, accident, accidental, accidents, accompanied, accord, accounting, accurate, accusative, achieve, achieves, acquainted--usually, acquiesce, acquiescence, acquisition, activity, acute, adapts, adding, address, addresses, adduced, adequacy, adequately, admirable, admit--though, admitted--that, admitting, admixture, advances, adversaries, advocate, advocates, advocating, aesthetic, aether, affairs, affections, affirmative, afforded, agent, agreeably, agreed--the, aim, aimed, akin, alien, all-embracing, all-important, all-round, allowed, allowing, alteration, altered, alternate, alternatives, alters, ambitious, amend, amply, analogous, analyse, analyses, anatomist, animal, announcement, anticipate, anticipating, anticipation, anybody's, appearances, appearing, appetite, application, applied, apportions, apprehended--must, apprehends, approaches, approaching, arduous, argued, argued--correctly, argues, arguing, arose, arrange, array, arrived, arriving, arrogant, artificial, ascertained, assassinated, assented, assigns, assimilate, assist, assumptions, assurance, assure, assured, astray, ate, atheists, atoms, attached, attack, attain, attainable, attained--many, attested, attitude, attributing, author, avoided, avoiding, aware--say, awayâ€”you, babies, bad--it, baldness, banging, bannerman, bar, barrier, be--at, be--is, be--something, bed, beg, begging, beginner, begs, begun, behave, behaviour, being--as, beleagured, beliefs--for, beliefs--such, believes--to, bell, bewildering, bibliographical, bid, bigger, binary, binds, bismarck's, blank, blue-green, bolder, bone, bored, borne, bounds, bradley, brick, briefest, brighter, brightness, british, broken, brotherhood, build, builder, buildings, calculate, calculated, calm, campbell, camps--friends, candid, cantor, card, careful, carelessly, carry, catalogue, catalogued, catching, categories, centuries, century, chance, changeable, changed, charitable, charities, checks, child, chimaera, choice, chosen, citizens, citizenship, clash, clashes, clean, clock, closed, closer, closes, clouds, co-operation, coalesce, codes, cogito, cohere, coin, coins, collective, colony, colour-blind, combination, combine, coming, commercial, commit, communicate, community, comparative, compare, compared, comparing, competent, compilation, complement, completed, compressed, conceivable, conceivably, conceiving, concentrate, concluded, concourse, condemns, conduct, confess, confine, confining, confirmation, confirmed, confirms, conflict, conformity, confused, confusing, confusions, confute, conjecture, connaÃ®tre, connecting, connects, consent, consequences, consequential, consisted, constituting, construct, constructed, constructing, constructions, constructive, contemplated, contemplating, contemplative, contemptuously, contended, contention, contentions, continental, continued, continuing, continuous, continuously, contract, contradict, contradictory, contrast, contributed, contribution, convenient, converged, conversation, convert, convincing, cool, corporation, correctly, correlative, corresponded, corrupt, counterparts, countless, countries, cranny, creates, creation, criticisms, criticized, crudely, current, custom, customary, cutting, damaged, danger, data--if, dates, debate, debated, deceitful, deceive, december, deceptive, decides, deciding, decision, declared, deducing, deductible, deeds, deeply, defeated, defects, defensible, defined, definiteness, definitions, deletions, deliberate, delightful, delusion, demanding, demonstrably, demonstrative, demonstratively, denies, dentist, dependence, depending, descend, describes, describing, descriptive, desert--it, deserts, deserves, deserving, designate, desirability, desirable, desiring, desirous, destroyed, destroys, destructive, detach, determines, development, developments, diary, dictionary, die, different--something, differing--some, diminish, diminished, diminishes, dined, dinner-table, directions, directs, disappear, disappoint, disappointing, disclaim, disclaimers, discontinue, discuss, disease, disengaged, disk, dismiss, dispassionately, display, displayed, disputable, disputants, dissociated, distinctly, distinguishes, distort, distorts, distributor, diverge, diversity, divest, divide, divided, divisibility--philosophers, dogmas, dogmatic, dogmatism, dominion, donation, donors, doubtless, downloading, drawn, dream-table, dreams--that, dried, drifted, drive, drives, duly, e-mail, easiest, east, eclipse, edition, educated, educational, effected, ein, elapse, elapsed, elect, elected, electric, elementary, elements, else--something, elucidation, embark, emerged, emerges, emitted, emitting, emphasized, emphatically, empiricists--who, employee, employees, employs, engagement, engendered, england, english, enlarged, enlarges, enormously, enquiry, enrich, ensuring, entails, entangled, enter, entertaining, enumerated, enumeration, enunciated, equal, equivocation, ergo, esse, estimated, etc.--is, etc.--may, etc.--which, eternal, eternally, euclid's, europe, everyday, exact, examines, example--raises, exceedingly, excluding, exclusion, exemplify, existent, expend, expense, experience--as, experience--not, explicitly, explored, exploring, exponents, exporting, expressions, extend, extends, extensive, extrinsic, fabric, face--it, facility, facts--infinite, faculty, failing, fainter, faintness, fallacies, fallacious--which, fallacy, fallible, falls, falsified, family, fancy, fast, fatal, favouritism, fear, features, fed, feeds, fetters, feverish, file, files, financial, fine, fitness, fixed, flash, foes, foolish, football, force, forces, forget, forgetting, forgo, forgotten, forks, formal, formally, formats, fortress, fortuitous, fortune, forwards, fostered, fourth, fragment, fragments, framework, friend, friends, frightened, fulfilment, fulfils, gain, garrison, generality, genuinely-empirical, geography, geology, georg, george, ghost-stories, gilbert, glance, glasses, globe, goals, goodwill, gordon, gradations, grain, grapple, grasp, gratefully, gratuitous, gravely, greeks, greens, greeny-blue, groundless, grow, growing, guarded, guide, gutenberg's, gutenbergâ„¢â€™s, habits, habitual, habitually, hair-splitting, halves, handbooks, happiness, hardnesses, harm, harmless, harmony, hart, hates, hatred, hatreds, heartbeats, heaven, heavens, hegel's, helpful, henry, heretofore, hesitatingly, hides, hills, hinder, historian, historic, historically, honour, hoofs, hoped, hopeless, horrors, host, hot, however--which, hume's, hume--maintained, hundreds, hypertext, ideas'--i.e, identification, identity, ides, ignorance, ignorant, iii, illumination, illusoriness, illusory, illustrated, illustrations, immanuel, immutable, impaired, impairs, impartial, impartially, impenetrable, impersonal, impose, impression, imprisoned, inaccurate, incidental, inclination, includes, incompatible, inconsistencies, inconsistency, increase, increasing, incredulous, indefinitely, indemnify, indemnity, independence, indestructible, indicating, indirect, inevitable, inexplicable, infallibility, infancy, infected, inferring, infinitesimal, infinitum, inflected, inflections, influence, infringed, infringement, inhabitants, inhabited, inherent, innocent, inoperative, inquire, inquired, insistent, insoluble, inspiration, instants, instincts, instrument, insubstantial, intelligent, intend, intended, intently, interact, interferes, intermediary, international, interpret, interpreted, interrelation, interrupted, introduce, introduction, intuitively, invalidity, invented, inventing, inventions, investigations, invites, involved--at, involved--in, irrefutable, irs, isolation, it--i, it--theoretically, itself',(1, jealousy, joint, june, justly, kantian, keener, kennen, keynes, killed, king's, knits, knives, knowable, knowledge--far, knowledge--knowledge, kÃ¶nigsberg, lake, lapse, larger, latin, launched, lay, learnt, legally, legitimately, leibniz--maintained, less--from, level, liberating, liberation, liberator, liberty, licensed, lies, life--which, life-long, light-waves, limitations, limiting, linked, lips, live, lived, locations, locke, logically--so, logician, long-lived, longest-lived, looked, loose, loosen, lot, loud, lowest, luminously, lying, machine-readable, magnifies, maintain, maintaining, majority, man-made, manager, many--profess, march, marching, marriage, match, maximum, meanings, meditations, melt, men's, mention, merchantability, mercilessly, merest, merges, merits, metaphorically, metaphysic, michael, middle, middle-aged, midst, miles, mind--not, mineralogist, minor, miracle, misery, misleadingness, mississippi, mistake, mistakes, misunderstanding, mitigate, mode, modification, modifications, modify, momentary, monad, monadism, monadology, monism, month, moore, moved, movements, multiplication, multiplicity, murray, musical, mutually, mystic, mystical, mysticism, nation, native, nearest, nearness, neglecting, negligence, negligible, network, newness, news, newsletter, newspapers, newton's, ninety-three, noises, nominative, non-existence, non-logical, non-profit, nonproprietary, nook, note, noteworthy, noticing, notifies, notions, nouns, novelist, nowadays, nowhen, numerous, object--in, objectionable, objective, objects--it, oblivious, obscure, observe, observing, obsolete, obstacle, obstacles, obstinate, obtuse, obviousness, obviousness--the, occupant, occupying, odd, office, oftener, omission, oneself, oneâ€”the, opaque, operated, operative, opportunities, opportunity, opposites, orator, orderly, ordnance, organized, organizing, organs, origin, originally, originator, outcome, outdated, outlines, outward, oval, owed, painfully, palpable, papers, paperwork, paradoxes, paradoxical, partially, partly--and, party, passage, passes, past--nor, past--not, pausing, pay, peace, peculiar, peculiarly, percipi, perfection, performances, periodic, permanence, perpetual, persons, perspective, persuaded, persuading, perturbed, pglaf, phantasmagoria, philosopher's, philosophers--or, philosophers--that, philosophize, philosophy--for, philosophy--the, physics, physiological, physiology, pink, places--london, placing, plane, planet, plausibility, playing, poisonous, positively, possess, possessed, possession, poverty, powerlessness, powers, practice, practised, pre-eminently, precisely, preface, preferable, preparing, prescribe, presence, preservation, pressing, pressure, pressures, presumption, presupposed, pretty, prevent, primitive, principles--perhaps, proceeded, proceeds, processes, processing, producing, production, products, profit--the, profitable, profited, profits, profoundest, progress, progressively, prohibition, prolegomena, prolonged, promote, promotion, pronounce, proofread, proposed, proprietary, provision, provisionally, provisions, prudent, prussia, psychologically, psychology, punitive, pursue, pushed, puzzling, questions--and, quibbling, range, ranging, rapping, rationalist, rationalists--who, rationality, readable, readers, real--or, realization, realized, reappear, reasons--in, receiving, recognizes, recombines, reconcile, reconstructed, recorded, redistribute, reduce, reducible, refined, reflecting, refutable, regress, regulating, rejection, relate, relation--in, relational, release, reliable, religion, remarkably, remarks, remedies, remote, remoter, removal, removes, renamed, renounce, repeat, repeatedly, repetition, replaced, reported, reports, represent, representations, representative, representing, republic, request, resemblances, residue, restate, resting, restricted, retain, retained, retort, returning, returns, revealed, revenue, reversed, revert, review, revolution, rigid, rival, rob, robbing, roman, roof, rooted, rotate, rotating, roundabout, rounded, rouse, roused, runs, s/he, salt, samples, satisfaction, satisfying, save, savoir, scaffold, sceptic, sceptical, sceptics, schoolmen, sea, searches, sections, secures, seldom, self-acquainted-with-sense-datum, self-interest, self-subsistent, self_-consciousness, send, sending, sensation--the, sense--thoughts, sense-data--brown, sense-data--colour, sense-data--for, sense-data--which, separated, sequel, serve, sets, seventeenth, severe, shadow, sharing, sharp, sheets, shiny, shock, short, shortly, simplify, sir, situated, skeleton, sleeping, slowly, smelling, smells, sober, society, sold, solicitation, solid, solve, solves, solving, somebody's, sophistry, soul, sour, south, space-relations, speak--that, specially, species, spectacles, speculative, speech, spoke, spoons, sprang, staff, stages, started, starts, stateâ€™s, step, stone, stored, straightforward, strangeness, stranger, strangest, strike, strikes, striking, strives, striving, struck, structure, struggling, struldbugs, studies, sublime, subscribe, subsequent, subsequently, subsumed, subsumptions, success, successively, suffering, sufficed, suggestion, suggestions, sum-total, sun's, sunset, supplied, supra-sensible, surprised, surrender, surrounded, surveys, survive, surviving, suspended, swamp, sweet, swift, synonymous, system--similarly, systematize, table--and, table--are, table--it, table--its, tablecloth, tactile, tap, task, tasting, taught, taxes, telegram, temper, temperaments, tempted, tending, tenet, term, texts, texture, the-letters, theoretical, theses, thesis, this--that, thraldom, throw, throws, thwarting, time-relations, timeless, tincture, title, tolerably, tongue, toothache, topics, total, touch--leaves, touches, trademark/copyright, traditional, trains, trammels, transcribe, transcription, transfer, transform, transitory, travels, treated, treating, treatment, trifling, trotting, trouble, troubled, troubles, trusted, tuning, type, types, tyranny, unaccustomed, unaffected, unattainable, uncertain, unchanged, unchanging, uncomfortable, uncommon, unconscious, undeniable, undeniably, underlie, underlies, understand--in, understanding, undertake, undiminished, undue, uneducated, unenforceability, unexpectedly, unexperienced, unimportant, unites, uniting, university, unlearn, unlink, unnecessary.(1, unplausible, unprotected, unpublished, unreflectingly, unreflective, unsafe, unsolicited, unsolved, unsound, unsupported, unsuspected, unusual, unwarrantable, unwise, upset, upstairs, urges, usage, ut, vagueness, vaguer, vain, valleys, value--perhaps, value--through, variable, variations, variety, variously, varying, vast, veil, verification, version, view--which, viewing, viii, vindicate, violates, violent, virtuous, virus, voice, voices, void, volume, volunteer, voyage, walk, walks, walled, walls, warrants, waste, waterloo, wear, wearing, weather, weight, whitehead, window, wise, wissen, with--in, wonderful, wooden, wore, worst, wrings, writers, www.gutenberg.org/contact, www.gutenberg.org/license, xii, xiii, xiv, xv, yield, york, â€˜as-isâ€™\n",
    "3. There are so many words with least probability and less probability. The probabilities are very less when we remove stopwords.\n",
    "4. If a belief is true, it can be deduced it is universal. with probability 0.0000000002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf a belief is true, it can be deduced it is universal.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m sentence2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCriticism of knowledge is counter to scientific results.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m combined_probability1 \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_combined_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_probabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m combined_probability2 \u001b[38;5;241m=\u001b[39m calculate_combined_probability(sentence2, word_probabilities)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m combined_probability1 \u001b[38;5;241m>\u001b[39m combined_probability2:\n",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m, in \u001b[0;36mcalculate_combined_probability\u001b[0;34m(sentence, word_probabilities)\u001b[0m\n\u001b[1;32m      7\u001b[0m combined_probability \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m      8\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mstopwords_file\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     10\u001b[0m     stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords_file' is not defined"
     ]
    }
   ],
   "source": [
    "# You will use the sum of the probabilities of each non-stop word to answer the question. You will need to give numeric rationale for your answer. Show your work in Python!\n",
    "import csv\n",
    "import string\n",
    "\n",
    "def calculate_combined_probability(sentence, word_probabilities):\n",
    "    words = sentence.split()\n",
    "    combined_probability = 1.0\n",
    "    stopwords = []\n",
    "    with open('stopwords-en.txt', 'r') as file:\n",
    "        stopwords = set(line.strip() for line in file)\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() not in stopwords:\n",
    "            cleaned_word = word.strip(string.punctuation)\n",
    "            probability = word_probabilities.get(cleaned_word.lower(), 0)\n",
    "            combined_probability *= probability\n",
    "\n",
    "    return combined_probability\n",
    "\n",
    "\n",
    "\n",
    "word_probabilities = {}\n",
    "csv_file = 'all_ns_words.csv'\n",
    "with open(csv_file, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        word = row[0]\n",
    "        probability = float(row[2])\n",
    "        word_probabilities[word] = probability\n",
    "\n",
    "sentence1 = \"If a belief is true, it can be deduced it is universal.\"\n",
    "sentence2 = \"Criticism of knowledge is counter to scientific results.\"\n",
    "\n",
    "combined_probability1 = calculate_combined_probability(sentence1, word_probabilities)\n",
    "combined_probability2 = calculate_combined_probability(sentence2, word_probabilities)\n",
    "\n",
    "if combined_probability1 > combined_probability2:\n",
    "    print(f\"Sentence 1 is more likely with a combined probability of {combined_probability1:.10f}.\")\n",
    "elif combined_probability2 > combined_probability1:\n",
    "    print(f\"Sentence 2 is more likely with a combined probability of {combined_probability2:.10f}.\")\n",
    "else:\n",
    "    print(f\"The sentences have the same combined probability of {combined_probability1:.10f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sau-fall-2023]",
   "language": "python",
   "name": "conda-env-sau-fall-2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
